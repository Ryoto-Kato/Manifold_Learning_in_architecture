{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold learning of images of buildings to identify a trend in architecture\n",
    "\n",
    "Interdisciplinary project at Technical University of Munihch (TUM CIT, MSc. Computer Science)\n",
    "\n",
    "@Chair of Urban Development, TUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected 21 architectural projects\n",
    "![21ProjectLists](./images/OriginalImage_sample21Images-3.png)\n",
    "- observation period: 1999-2019\n",
    "- chose one project per each awardee (in total, 21 architectural projects)\n",
    "- We chose twenty-one architectural projects, primarily cultural institutions such as auditoriums, conference centres, museums, and schools, designed by Pritzker Prize laureates between 1999 and 2019.\n",
    "\n",
    "\n",
    "![SampleImages](./images/table21Projects-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Overview\n",
    "1. Loading dataset (images with additional information)\n",
    "    - Image scraper\n",
    "    - Image loader\n",
    "2. Image pre-processing\n",
    "    - Image transformer\n",
    "    - Difference of Gaussians creater\n",
    "3. Manifold learning\n",
    "    - Autoencoder\n",
    "4. Dimensionality reduction\n",
    "    - LDA dimensionality reduction\n",
    "5. Visualization of results\n",
    "    - Visualizer of a latent structure mapping\n",
    "\n",
    "![SystemOverView](./images/SystemOverview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment\n",
    "- Python 3.10.9\n",
    "    - Requirements: pip-install requirement.txt\n",
    "- Manjaro Linux\n",
    "- Anaconda 22.9.0\n",
    "    - Python 3.10.9\n",
    "    - Package list (check ./requirements.txt)\n",
    "- Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of 21 architectural projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOf21ap = [\"Carré d'Art\", \"Educatorium\", \"Tate Modern\", \"Arthur and Yvonne Boyd Education Center\", \"Sydney Opera House\", \"Contemporary Arts Center\", \"Diamond Ranch High School\", \"São Paulo Museum of Art\", \"Centre Pompidou\", \"Guthrie Theater\", \"Kunsthaus Bregenz\", \"New Museum\", \"Casa das Histórias Paula Rego\", \"Ningbo History Museum\", \"Toyo Ito Museum of Architecture, Imabari\", \"Centre Pompidou-Metz\", \"Munich Olympic Park\", \"Siamese Towers\", \"Musée Soulages\", \"Tagore Memorial Hall\", \"Qatar National Convention\"]\n",
    "\n",
    "DictOf21ap = {}\n",
    "start_year = 1999\n",
    "\n",
    "for i in range(len(ListOf21ap)):\n",
    "    DictOf21ap.update({i+start_year : \"\"})\n",
    "\n",
    "print(\"The year of award: The name of building\\n\")\n",
    "for index, key in enumerate(DictOf21ap.keys()):\n",
    "    DictOf21ap[key] = ListOf21ap[index]\n",
    "    print(f'{key}: {DictOf21ap[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image scraper\n",
    "- Scrape images from Microsoft Bing and save in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Image_scraper import Building\n",
    "from utils.Image_scraper import Bing_image_Scraper\n",
    "from utils.Image_scraper import Instant_BingImageScraper\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.Archtecture_dataloader import ImageDataset, Memory_ImageDataset\n",
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "from utils.transformers import GrayScaler, RescaleTransform, compute_image_mean_and_std, NormalizeTransform, ComposeTransform, Center_crop_by_small_axis\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "- Scrape images from Microsoft Bing with keywords in the above ListOf21ap (The name of buildings)\n",
    "- Transform the image into 256x256 pixels by using torch vision\n",
    "- Save them to the respective folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformer\n",
    "resizer = transforms.Resize(size = 256)\n",
    "transformer = ComposeTransform([transforms.ToTensor(), Center_crop_by_small_axis, resizer, transforms.ToPILImage()])\n",
    "\n",
    "# path to directory to save transformed images\n",
    "path_to_directory = os.path.join(os.getcwd(), 'images', '21projects_selected')\n",
    "\n",
    "if not os.path.exists(path_to_directory):\n",
    "    os.mkdir(path_to_directory)\n",
    "\n",
    "index = 0\n",
    "\n",
    "if os.path.exists(path_to_directory):\n",
    "    print(f\"You have already scraped images from Microsoft Bing, \\ncheck {path_to_directory}\")\n",
    "else:\n",
    "    for bname in tqdm(ListOf21ap, position=0, leave=True):\n",
    "        # print(f\"-----------{bname}-------------\")\n",
    "        # print(f\"1: Scraping images of {bname}...\")\n",
    "        _set = Instant_BingImageScraper(b_name=bname, num_images=100) #The maximum number of images we can scrape from Microsoft Bing is 150 at once\n",
    "        # print(f\"1: Done.\")\n",
    "        \n",
    "        # print(f\"Transforming images...\")\n",
    "        _transformed_images = []\n",
    "        for im in _set.images:\n",
    "            _im = transformer(im)\n",
    "            _transformed_images.append(_im)\n",
    "        # print(f\"Done.\")\n",
    "\n",
    "        path_to_folder = os.path.join(path_to_directory, str(index))\n",
    "        # print(f\"Saving images in to the {path_to_folder}...\")\n",
    "        if not os.path.exists(path_to_folder):\n",
    "            os.mkdir(path_to_folder)\n",
    "        for i, pil_im in enumerate(_transformed_images):\n",
    "            _im_name = str(i) + \".jpg\"\n",
    "            pil_im.save(os.path.join(path_to_folder, _im_name))\n",
    "        # print(f\"Done.\")\n",
    "        # print(f\"------------------------------\")\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Image retrieval\n",
    "- Retrieve images so that I have only images that can represent the respective buildings' appearance from the outside Refer the Pritzker Prize Official Website (https://www.pritzkerprize.com/)\n",
    "- The result of this process could be dounloaded from here (https://drive.google.com/file/d/1YwiuVKi9BFPrZdQq6uu48CD1vVNTvlfy/view?usp=sharing)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from utils.Image_scraper import Building\n",
    "from utils.Image_scraper import Bing_image_Scraper\n",
    "from utils.Image_scraper import Flicker_image_Scraper\n",
    "from utils.Archtecture_dataloader import ImageDataset, Memory_ImageDataset\n",
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "from utils.transformers import GrayScaler, RescaleTransform, compute_image_mean_and_std, NormalizeTransform, ComposeTransform, Center_crop_by_small_axis\n",
    "\n",
    "# autoencoder and t_sne\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_Imfolder = os.path.join(os.getcwd(), 'images', '21projects_selected')\n",
    "print(path_to_Imfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte2image(self, bi_im):\n",
    "    \"\"\"convert byte to image\n",
    "    Args:\n",
    "    ----\n",
    "    bytes_im:  byte\n",
    "               byte image\n",
    "    Returns:\n",
    "    -------\n",
    "    Image: Image    \n",
    "    \"\"\"\n",
    "    im = Image.open(io.BytesIO(bi_im))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_images = {}\n",
    "\n",
    "for i in range(21):\n",
    "    dict = {i : []}\n",
    "    dict_images.update(dict)\n",
    "\n",
    "for i in tqdm(range(21), position = 0, leave = True):\n",
    "    images = []\n",
    "    folder_name = str(i)\n",
    "    path_to_currentFolder = os.path.join(path_to_Imfolder, folder_name,)\n",
    "    # print(path_to_currentFolder)\n",
    "    for im in os.listdir(path_to_currentFolder):\n",
    "        # print(im)\n",
    "        _im = Image.open(os.path.join(path_to_currentFolder, im))\n",
    "        _array_im = np.asarray(_im)\n",
    "        images.append(_array_im)\n",
    "    dict_images[i]=images\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in dict_images[0]:\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num of images for each building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = 1e10\n",
    "for i in range(21):\n",
    "    print(\"class:\", i+1, len(dict_images[i]))\n",
    "    if min > len(dict_images[i]):\n",
    "        min = len(dict_images[i])\n",
    "\n",
    "print(\"The number of minimum: \", min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NofIm = 35"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump images in .pckl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "\n",
    "_save_path_to_memory = os.path.join(os.getcwd(), 'memory')\n",
    "_p_name = 'selected_Numpyimages_of_21buildings.pckl'\n",
    "if not os.path.exists(os.path.join(_save_path_to_memory, _p_name)):\n",
    "    dump_pckl(dict_images, _save_path_to_memory, _p_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images from .pckl\n",
    " - These images are already resized to 256x256 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_path_to_pckl = os.path.join(os.getcwd(), 'memory')\n",
    "_p_name_of_pckl = \"selected_Numpyimages_of_21buildings.pckl\"\n",
    "_memory_dict_images = load_from_memory(path_to_memory= _path_to_pckl, pickle_fname=_p_name_of_pckl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pytorch version installed: {torch.__version__}\")\n",
    "print(f\"pytroch vision version installed: {torchvision.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(f\"Original Device:  {x.device}\")\n",
    "\n",
    "tensor = x.to(device)\n",
    "print(f\"Current device: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the cuda availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    raise ValueError('GPU is unavailable, Reboot your machine')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "- Loading additional data for each image\n",
    "    - Download .pckl which contains following information from      \n",
    "    - Download link is [here](https://drive.google.com/file/d/1oUmoFhDsHwFjZ1wdDvi9HkNChxNLU7-e/view?usp=sharing) and save it ./memory\n",
    "- Contents \n",
    "    - The year of award [1999, 2019]\n",
    "    - The year of inauguration\n",
    "    - The category of cultural institution\n",
    "    - The name of building\n",
    "- Creating complete data set for manifold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_path_to_dataset = os.path.join(os.getcwd(), 'memory')\n",
    "_pickle_fname = 'ImageDataset_Bing_1805_normalized_squarecrop.pckl'\n",
    "\n",
    "rescaler = RescaleTransform()\n",
    "gray_scaler = transforms.Grayscale()\n",
    "mean = [0.47784522, 0.4758804, 0.47578678]\n",
    "std = [0.29073316, 0.28931433, 0.2894705]\n",
    "\n",
    "memory_dataset = Memory_ImageDataset(\n",
    "    path_to_memory=_path_to_dataset,\n",
    "    pickle_fname= _pickle_fname,\n",
    "    transform=ComposeTransform([transforms.ToTensor(), transforms.Resize(256), transforms.Normalize(mean, std), gray_scaler])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample a item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = memory_dataset[0]\n",
    "item_bname = sample_item['name']\n",
    "print(item_bname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of name\n",
    "\n",
    "bname2index = {}\n",
    "counter = 0\n",
    "for i, data in enumerate(memory_dataset):\n",
    "    if data['name'] not in bname2index:\n",
    "        _dict = {data['name']: counter}\n",
    "        bname2index.update(_dict)\n",
    "        counter = counter + 1\n",
    "print(bname2index.keys())\n",
    "\n",
    "bname_list = {}\n",
    "category_list = {}\n",
    "estbyear_list = {}\n",
    "awardyear_list = {}\n",
    "\n",
    "for i, data in enumerate(memory_dataset):\n",
    "    _index = bname2index[data['name']]\n",
    "    if data['name'] not in bname_list:\n",
    "        _dbname = {_index: data['name']}\n",
    "        bname_list.update(_dbname)\n",
    "    if data['category'] not in category_list:\n",
    "        _dcategory = {_index: data['category']}\n",
    "        category_list.update(_dcategory)\n",
    "    if data['year of establishment'] not in estbyear_list:\n",
    "        _destbyear = {_index: data['year of establishment']}\n",
    "        estbyear_list.update(_destbyear)\n",
    "    if data['award_year'] not in awardyear_list:\n",
    "        _dawardyear = {_index: data['award_year']}\n",
    "        awardyear_list.update(_dawardyear)\n",
    "print(bname_list.items())\n",
    "print(category_list.items())\n",
    "print(estbyear_list.items())\n",
    "print(awardyear_list.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_building_name = []\n",
    "for key in bname_list.keys():\n",
    "    list_building_name.append(bname_list[key])\n",
    "print(list_building_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_per_class = NofIm\n",
    "\n",
    "c_images = []\n",
    "c_labels = []\n",
    "c_categories = []\n",
    "c_estbyear = []\n",
    "c_awardyear = []\n",
    "\n",
    "for i, bname in enumerate(list_building_name):\n",
    "    _index = bname2index[bname]\n",
    "    label = bname_list[_index]\n",
    "    category = category_list[_index]\n",
    "    estbyear = estbyear_list[_index]\n",
    "    awardyear = awardyear_list[_index]\n",
    "    counter = 0\n",
    "    index_random_list = np.random.permutation(35)\n",
    "    for idx, rand_id in enumerate(index_random_list):\n",
    "        im = _memory_dict_images[_index][rand_id]\n",
    "        if counter >= num_image_per_class:\n",
    "            break\n",
    "        _transformed_im = memory_dataset.transform(im)\n",
    "        im = np.asarray(_transformed_im)\n",
    "        c_images.append(im)\n",
    "        c_labels.append(label)\n",
    "        c_categories.append(category)\n",
    "        c_estbyear.append(estbyear)\n",
    "        c_awardyear.append(awardyear)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_building_name = []\n",
    "for key in bname_list.keys():\n",
    "    list_building_name.append(bname_list[key])\n",
    "print(list_building_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_memory_dict_images.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoGs creation\n",
    "- DoG: Difference of Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example for creation of Gaussian blured image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c_images[0].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "print(c_images[0].mean())\n",
    "dog_sample1 = cv2.GaussianBlur(c_images[0], (11,11), 0)\n",
    "plt.imshow(dog_sample1.squeeze(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creation of Difference of Gaussians for each images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = []\n",
    "for im in c_images:\n",
    "    im = im\n",
    "    low_sigma = cv2.GaussianBlur(im, (3,3), 0)\n",
    "    high_sigma = cv2.GaussianBlur(im, (11,11), 0)\n",
    "    _dog = low_sigma - high_sigma\n",
    "    max = _dog.max()\n",
    "    min = _dog.min()\n",
    "    _dog = (_dog-min)/(max-min)\n",
    "    dogs.append(_dog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of an example of Difference of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dogs[0].squeeze(), 'gray')\n",
    "print(dogs[0].max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output size of conv2Dtransposed()\n",
    "\n",
    "def get_ConvTransoutputSize(input_size, padding, kernel_size, stride):\n",
    "    return (input_size-1)*stride - (2*padding) + (kernel_size-1)+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check input and output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "id_lists = []\n",
    "id_list_test = []\n",
    "\n",
    "original_x_train = []\n",
    "x_train = []\n",
    "y_train = []\n",
    "train_year = []\n",
    "train_estbyear = []\n",
    "train_categories = []\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "original_x_valid = []\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "valid_year = []\n",
    "valid_estbyear = []\n",
    "valid_categories = []\n",
    "\n",
    "original_x_test = []\n",
    "x_test = []\n",
    "y_test =[]\n",
    "test_year = []\n",
    "test_estbyear = []\n",
    "test_categories = []\n",
    "\n",
    "image_dataset = dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_image_per_class = len(image_dataset)\n",
    "print(train_num_image_per_class)\n",
    "while(j<len(image_dataset)):\n",
    "    for s in range(NofIm):\n",
    "        id_lists.append(j+s)\n",
    "        if(s<20):\n",
    "            original_x_train.append(image_dataset[j+s])\n",
    "            x_train.append(image_dataset[j+s])\n",
    "            y_train.append(list_building_name[i])\n",
    "            train_year.append(c_awardyear[j+s])\n",
    "            train_estbyear.append(c_estbyear[j+s])\n",
    "            train_categories.append(c_categories[j+s])\n",
    "        elif(s >= 20 and s < 25):\n",
    "            original_x_valid.append(c_images[j+s])\n",
    "            x_valid.append(image_dataset[j+s])\n",
    "            y_valid.append(list_building_name[i])\n",
    "            valid_year.append(c_awardyear[j+s])\n",
    "            valid_estbyear.append(c_estbyear[j+s])\n",
    "            valid_categories.append(c_categories[j+s])\n",
    "        elif(s >= 25):\n",
    "            original_x_test.append(c_images[j+s])\n",
    "            x_test.append(image_dataset[j+s])\n",
    "            y_test.append(list_building_name[i])\n",
    "            test_year.append(c_awardyear[j+s])\n",
    "            test_estbyear.append(c_estbyear[j+s])\n",
    "            test_categories.append(c_categories[j+s])\n",
    "    i = i + 1\n",
    "    j = NofIm * i\n",
    "\n",
    "single_class = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training data set\")\n",
    "print(len(x_train))\n",
    "print(\"validation data set\")\n",
    "print(len(x_valid))\n",
    "print(\"test data set\")\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(x_train, batch_size = 16, shuffle = True, pin_memory=True, num_workers = 2)\n",
    "valid_dataloader = torch.utils.data.DataLoader(x_valid, batch_size = 16, shuffle = False, pin_memory=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize sample DoG from training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].squeeze(), 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check length of respective data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataloader.__len__())\n",
    "print(valid_dataloader.__len__())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declaration of convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=1, bias=False),\n",
    "        # nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "![NetworkArchitecture](./images/NetworkArchitecture_UnetwithInOut.png)\n",
    "\n",
    "## Layers\n",
    "![Layers](./images/Layers_network-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder1 = double_conv(1, 32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding= 0, dilation=1, ceil_mode=False)\n",
    "        self.encoder2 = double_conv(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding= 0, dilation=1, ceil_mode=False)\n",
    "        self.encoder3 = double_conv(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding= 0, dilation=1, ceil_mode=False)\n",
    "        self.encoder4 = double_conv(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding= 0, dilation=1, ceil_mode=False)\n",
    "        self.bottleneck = double_conv(256, 512)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride =2)\n",
    "        self.decoder4 = double_conv(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride = 2)\n",
    "        self.decoder3 = double_conv(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size= 2, stride = 2)\n",
    "        self.decoder2 = double_conv(128, 64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size = 2, stride = 2)\n",
    "        self.decoder1 = double_conv(64, 32)\n",
    "        self.last_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, 1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.weight_skip = 0.001\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv1 = self.encoder1(x)\n",
    "        x = self.pool1(conv1)\n",
    "\n",
    "        conv2 = self.encoder2(x)\n",
    "        x = self.pool2(conv2)\n",
    "\n",
    "        conv3 = self.encoder3(x)\n",
    "        x = self.pool3(conv3)\n",
    "\n",
    "        conv4 = self.encoder4(x)\n",
    "        x = self.pool4(conv4)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        up1 = self.upconv4(x)\n",
    "        x = self.decoder4(torch.cat([conv4, up1], 1))\n",
    "\n",
    "        up2 = self.upconv3(x)\n",
    "        x = self.decoder3(torch.cat([conv3, up2], 1))\n",
    "\n",
    "        up3 = self.upconv2(x)\n",
    "        x = self.decoder2(torch.cat([conv2, up3], 1))\n",
    "\n",
    "        up4 = self.upconv1(x)\n",
    "        x = self.decoder1(torch.cat([conv1, up4], 1))\n",
    "\n",
    "        out = self.last_conv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the function for weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.xavier_normal_(module.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Autoencoder\n",
    "configuration\n",
    "\n",
    "![Config](./images/Configuration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ExponentialLR(optimizer, gamma = 0.999)\n",
    "log_every_batch = 20\n",
    "max_epochs = 25\n",
    "avg_train_loss = []\n",
    "avg_test_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training and validation at each 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss_trace = []\n",
    "    for batch, x in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        # predict\n",
    "        predict = model(x)\n",
    "        predict = predict.clamp(0, 1)\n",
    "        predict[predict!=predict] = 0\n",
    "        # evaluate reconstruction loss (Binary cross entropy)\n",
    "        loss = F.mse_loss(predict, x)\n",
    "        # set 0 to the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # calculate gradients by backward propagation\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # loss.detach() => separate a loss from the computational graph, and doesn't require gradient\n",
    "        # detached tensor == tensor\n",
    "        # tensor.item() == content\n",
    "        train_loss_trace.append(loss.detach().item())\n",
    "        if batch % log_every_batch == 0:\n",
    "            print('Train: Epoch {}, batch {}, ====> loss {}'.format(epoch, batch, loss))\n",
    "    \n",
    "    # if you do not call loss.backward()\n",
    "    # with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss_trace = []\n",
    "        for batch, x in enumerate(valid_dataloader):\n",
    "            x = x.to(device)\n",
    "            predict = model(x)\n",
    "            predict = predict.clamp(0, 1)\n",
    "            predict[predict!=predict] = 0\n",
    "            loss = F.mse_loss(predict, x)\n",
    "            test_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print('Test: Epoch {}, batch {}, ====> loss {}'.format(epoch, batch, loss))\n",
    "    scheduler.step()\n",
    "    _avg_train_loss = np.mean(train_loss_trace)\n",
    "    avg_train_loss.append(_avg_train_loss)\n",
    "    _avg_test_loss = np.mean(test_loss_trace)\n",
    "    avg_test_loss.append(_avg_test_loss)\n",
    "    print(f\"Epoch {epoch} finished -average train loss {_avg_train_loss}, \"\n",
    "    f\"average test loss {_avg_test_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array(avg_train_loss)\n",
    "test_loss = np.array(avg_test_loss)\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Autoencoder\")\n",
    "\n",
    "plt.plot(train_loss, label = 'train')\n",
    "plt.plot(test_loss, label = 'test')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition of low-dimensional latent feature representation of test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(x_test, batch_size = 64, shuffle = False, pin_memory=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_dataloader = torch.utils.data.DataLoader(y_test, batch_size = 64, shuffle = False, pin_memory=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of test images\n",
    "test_num_image_per_class = int(len(x_test)/21)\n",
    "print(test_num_image_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain low-dimensional latent feature representation for test data set\n",
    "- Use encoder part of trained autoencoder\n",
    "- Obtain embeddings from bottleneck layer\n",
    "- Obtain reconstructed images (make data set for further analysis of the quality of trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_image = []\n",
    "eval_predict = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent = []\n",
    "    for batch, x in enumerate(test_dataloader):\n",
    "        layer_1 = model.encoder1(x.to(device))\n",
    "        layer_2 = model.pool1(layer_1)\n",
    "        layer_3 = model.encoder2(layer_2)\n",
    "        layer_4 = model.pool2(layer_3)\n",
    "        layer_5 = model.encoder3(layer_4)\n",
    "        layer_6 = model.pool3(layer_5)\n",
    "        layer_7 = model.encoder4(layer_6)\n",
    "        layer_8 = model.pool4(layer_7)\n",
    "        layer_9 = model.bottleneck(layer_8)\n",
    "        latent.append(layer_9.cpu())\n",
    "        eval_image.append(x)\n",
    "        predict = model(x.to(device)).cpu()\n",
    "        eval_predict.append(predict)\n",
    "    latent = torch.cat(latent)\n",
    "    eval_image = torch.cat(eval_image)\n",
    "    eval_predict = torch.cat(eval_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize/Save reconstruction results\n",
    "Configure the path to save reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_figure = True\n",
    "# save_figure = False\n",
    "_path_to_image_folder = \"\"\n",
    "import os\n",
    "\n",
    "ex_name = 'Name_of_experiment'\n",
    "\n",
    "if _save_figure:\n",
    "   _path_to_image_folder = os.path.join(os.getcwd(), 'images', 'Results',ex_name)\n",
    "   if not os.path.exists(_path_to_image_folder):\n",
    "      os.makedirs(_path_to_image_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create/Save/(Visualize) a image for each input-output pair to visualize next to each other for each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eval_image)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    _b_name = y_test[i]\n",
    "    ax1.imshow(eval_image[i].squeeze(), cmap='gray')\n",
    "    ax1.set_title(\"Input DoG\")\n",
    "    ax2.imshow(eval_predict[i].squeeze(), cmap='gray')\n",
    "    ax2.set_title(\"Reconstructed DoG\")\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    if _save_figure:\n",
    "        b_name = _b_name+str(i%10)+\".png\"\n",
    "        path_to_folder = os.path.join(_path_to_image_folder, \"Autoencoder_result\")\n",
    "        if not os.path.exists(path_to_folder):\n",
    "            os.makedirs(path_to_folder, exist_ok=True)\n",
    "        f_name = os.path.join(path_to_folder, b_name)\n",
    "        plt.savefig(f_name)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation of id list of a pair of latent feature representation (1024 dims) and label\n",
    "- label is going to be the name of building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis_samples = len(x_test)\n",
    "print(n_vis_samples)\n",
    "indices = np.random.choice(len(latent), n_vis_samples, replace = False)\n",
    "print(indices)\n",
    "vis_samples = latent[indices]\n",
    "latent_y = []\n",
    "latent_binary = []\n",
    "latent_year = []\n",
    "for id in indices:\n",
    "    latent_y.append(y_test[id])\n",
    "print(vis_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction and visualization\n",
    "## Linear discriminant analysis dimensionality reduction\n",
    "For my research purpose, I employed linear discriminant analysis (LDA) dimensionality reduction as my dimensionality reduction algorithm for further data processing for latent structure visualisation and its interpretation. LDA dimensionality reduction is one of the supervised learning algorithms which is often used for classification tasks in machine learning.\n",
    "\n",
    "LDA dimensionality reduction can be derived from a generative model of a probabilistic classification modelling algorithm based on Baye's rule. A generative model is a probabilistic model which infers the class of a sample by sampling data from a predicted class distribution. Thus, this prediction can be obtained by posterior probability as follows.\n",
    "\n",
    "$$\\mathbf{P}(y=c|\\mathbf{x}, \\mu, \\Sigma, \\Pi) =  \\frac{\\mathit{P}(\\mathbf{x}|y=c)\\mathit{P}(y=c|\\Pi_{c})}{\\sum_{l=1}^{21}\\mathit{P(\\mathbf{x}|y=l)}\\textit{P}(y =l|\\Pi_{l})}\n",
    "= \\frac{exp(w_c^T\\mathbf{x} + w_{c0})}{\\sum_{l=1}^{21}exp(w_l^T\\mathbf{x} + w_{l0})}$$\n",
    "\n",
    "This likelihood $\\mathit{P}(\\mathbf{x}|y=c)$can be expressed given the mean and covariance of class $\\mathit{c}$ as prior knowledge as follows. \n",
    "\n",
    "$$\\mathbf{P}(x|y=c) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_c)^T\\Sigma^{-1}(x-\\mu_c))$$\n",
    "\n",
    "$$\\mu_{c} : \\text{mean of c's class distribution} \\quad\n",
    "\\Sigma : \\text{shared covariance matrix} \\quad\n",
    "\\Pi_{c} : \\text{prior knowledge of class c}\n",
    "$$\n",
    "\n",
    "Where I denote an extracted latent feature from bottleneck of encoder of trained neural network with $\\mathbf{x}_i = Encoder(\\mathbf{IM}_i)$ that has 1024 dimensions where $\\mathbf{IM}_i$ represents original image data with 65536 dimensions, I assign $\\mathbf{x}_i$ to the class \\textit{c} whose $\\mu_c$ is the closest in term of Mahalanobis distance that can be obtained by calculating $(x-\\mu_c)^T\\Sigma^{-1}(x-\\mu_c)$, while also accounting for the class prior probabilities. To this end, I assign $\\mathbf{x}_i$ to the class \\textit{c} which minimizes the negative log posterior. The negative log posterior is expressed as follows. \n",
    "\n",
    "$$-log\\mathit{P}(y=c|\\mathbf{x}, \\mu, \\Sigma, \\Pi) = -\\sigma(\\alpha)$$\n",
    "\n",
    "$$\\alpha =  w_c^T\\mathbf{x} + w_{c0} + Constant$$\n",
    "\n",
    "$$w_c = \\Sigma^{-1}\\mu_{c} \\quad w_{c0} = -\\frac{1}{2}\\mu_c^T\\Sigma^{-1}\\mu_{c} + log\\Pi_c$$\n",
    "\n",
    "Thus, the probability $\\mathbf{P}(y_i = c|\\mathbf{x_i})$ which represents how likely $\\mathbf{x}_i$ belongs to class \\textit{c} could be obtained by using maximum a prior as following.\n",
    "\n",
    "$$\\hat{\\mu}, \\hat{\\Sigma}, \\hat{\\Pi} = \\mathit{argmin} \\sum_{i=1}^{N = 210}\\sum_{c=1}^{C=21}-log[\\mathit{P}(\\mathbf{x}_i|\\mu_c, \\Sigma)\\mathit{P}(y_i = c | \\Pi)]$$\n",
    "\n",
    "$$\\hat{w_c} = \\hat{\\Sigma}^{-1}\\hat{\\mu_{c}} ,\\quad \\hat{w_{c0}} = -\\frac{1}{2}(\\hat{\\mu_{c}})^T\\hat{\\Sigma}^{-1}\\hat{\\mu_{c}}+ log\\hat{\\Pi_{c}}$$\n",
    "\n",
    "$$\\hat{\\alpha} = (\\hat{w_c})^T\\mathbf{x} + \\hat{w_{c0}}$$\n",
    "\n",
    "$$\\mathbf{P}(y_i = c|\\mathbf{x_i}) = \\sigma(\\hat{\\alpha})$$\n",
    "\n",
    "I finally assign x to the class $\\hat{y_i}$ by choosing the class c such that it maximizes the posterior probability $\\mathbf{P}(y_i = c|\\mathbf{x}_i)$. \n",
    "\n",
    "$$\\hat{y_i} = argmax_c\\mathbf{P}(y_i = c|\\mathbf{x}_i)$$\n",
    "\n",
    "Since I can also assume that every cluster has isotropic distribution, namely $\\Sigma = \\mathbf{I}$, this process is equivalent to assigning $\\mathbf{x}_i$ to the closest mean in terms of Euclidean distance. In the L2-norm sense, computing distance between $\\mu_{c}$ and $x_{i}$ is equivalent to taking distance after I project them onto affine subspace $\\mathcal{H}$ of original space. In other words, if $\\mathbf{x}_i$ is close to $\\mu_{c}$ in 1024 dimensional space, it will be preserved in the lower dimensional space as well. By choosing my target dimension $\\mathcal{L} = 2$ for visualization, I will find mapping function $F(\\mathbf{x}_i)$ which can maximize the variance of $\\mu_c^{*}$ after mapping from 1024 dimension to 2 dimensions as following.\n",
    "\n",
    "$$F^{*} = max\\frac{1}{C}\\sum_{c=1}^{C}(F(\\hat{\\mu_c}))^2$$\n",
    "\n",
    "$$\\textbf{where} \\quad F^{*}(\\hat{\\mu_c}) = \\hat{\\mu_c}^{*}$$\n",
    "\n",
    "Finally, I obtain 2-dimensional intrinsic feature representations for each sample $\\mathbf{x}_i$ for $i = \\{1,2,...,210\\}$ from my test data set as follows.\n",
    "\n",
    "$$F^{*}(\\mathbf{x}_i) = \\mathbf{x}_i^{*}$$\n",
    "\n",
    "Since LDA dimensionality reduction attempts to find directions that can separate different classes in the feature space (latent space) as I mentioned, this approach allows us to conduct semantic understanding among twenty-one architectural projects in terms of architectural designs by providing a meaningful low-dimensional representation of data structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_list={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "data = vis_samples.reshape(n_vis_samples, -1)\n",
    "coords_LDA = LinearDiscriminantAnalysis(n_components=2).fit_transform(data, latent_y)\n",
    "coordinates_list[\"lda\"] = coords_LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a category-index correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coordinates_list.keys())\n",
    "category2idx = {}\n",
    "idx2category = {}\n",
    "category_list = []\n",
    "for categ in c_categories:\n",
    "    if(categ not in category_list):\n",
    "        category_list.append(categ)\n",
    "\n",
    "for i, c in enumerate(category_list):\n",
    "    idx2category.update({i: c})\n",
    "    category2idx.update({c: i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas for latent structure visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas\n",
    "import pandas as pd\n",
    "datasets = {}\n",
    "dict = {}\n",
    "index = 0\n",
    "for key in coordinates_list.keys():\n",
    "    _x=[]\n",
    "    _y=[]\n",
    "    _valid_year=[]\n",
    "    _estb_year =[]\n",
    "    _category = []\n",
    "    _original_image = []\n",
    "    _dog_image = []\n",
    "    _label = []\n",
    "    _indices = []\n",
    "    for idx, (x, y) in zip(indices, coordinates_list[key]):\n",
    "        _x.append(x)\n",
    "        _y.append(y)\n",
    "        _indices.append(idx)\n",
    "        _label.append(y_test[idx])\n",
    "        _original_image.append(original_x_test[idx])\n",
    "        _dog_image.append(x_test[idx])\n",
    "        _valid_year.append(int(test_year[idx]))\n",
    "        _estb_year.append(int(test_estbyear[idx]))\n",
    "        _category.append(category2idx[test_categories[idx]])\n",
    "    _dataset = {'id': _indices, 'name': _label,'original_image': _original_image, 'dog_image': _dog_image, 'award_year': _valid_year, 'est_year': _estb_year, 'category': _category, 'x': _x, 'y': _y}\n",
    "    _pd = pd.DataFrame(_dataset)\n",
    "    datasets[key]=_pd\n",
    "    _dict = {'id': _indices, 'name': _label,'original_image': _original_image, 'dog_image': _dog_image, 'award_year': _valid_year, 'est_year': _estb_year, 'category': _category, 'coordinate': coordinates_list[key]}\n",
    "    dict[key] = _dict\n",
    "    print(_pd)\n",
    "    index = index + 1\n",
    "\n",
    "datasets\n",
    "data_attribute_label_list = []\n",
    "for label in _dataset.keys():\n",
    "    data_attribute_label_list.append(label)\n",
    "data_attribute_label_list = data_attribute_label_list\n",
    "print(data_attribute_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of obtained latent feature representation\n",
    "- Latent structure mapping with labels\n",
    "    1. Green(Summer): the year of award\n",
    "    2. Red(Autumn): the year of establish\n",
    "    3. Blue(Winter): the category of institute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import category_scatter\n",
    "\n",
    "colormap_list = ['summer', 'autumn', 'winter']\n",
    "data_attribute_label_list=['award_year', 'est_year', 'category']\n",
    "keys = [key for key in datasets.keys()]\n",
    "for key in datasets.keys():\n",
    "    for id, label in enumerate(data_attribute_label_list):\n",
    "        fig= datasets[key].plot.scatter(x='x', y='y', c=label, colormap=colormap_list[id], title = key)\n",
    "        if key == 'lda' and label == 'award_year':\n",
    "            b_name = \"LDA_ScatterLatentMap.png\"\n",
    "            f_name = os.path.join(_path_to_image_folder, b_name)\n",
    "            plt.savefig(f_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the latent feature representations for each input in pickel file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_memory = os.path.join(os.getcwd(), 'memory')\n",
    "p_name = ex_name + '_pandas_datasets_latentcoordinates.pckl'\n",
    "dump_pckl(datasets, path_to_memory, p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the latent feature representations in .pickl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_memory = os.path.join(os.getcwd(), 'memory')\n",
    "p_name = ex_name +'_data_dict.pckl'\n",
    "dump_pckl(dict, path_to_memory, p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the latent feature representations from .pockl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_temp_path_to_memeory = os.path.join(os.getcwd(), 'memory')\n",
    "_p_name = ex_name+'_data_dict.pckl'\n",
    "_memory_dict = load_from_memory(path_to_memory=_temp_path_to_memeory, pickle_fname=_p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data set\n",
    "- Gray-scale image vs. DoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def latent_space_ALLDatasetVis(data_dict, technique: str, save_figure, path_to_image_folder):\n",
    "    indices = data_dict['id']\n",
    "    coordinate = data_dict['coordinate']\n",
    "    bname_list = data_dict['name']\n",
    "    images = data_dict['original_image']\n",
    "    dog_images = data_dict['dog_image']\n",
    "    _award_year_list = data_dict['award_year']\n",
    "    # 'original_image': _original_image, 'dog_image':\n",
    "    # target_images = []\n",
    "    # target_dogimages = []\n",
    "    # _path_to_image_folder = os.path.join(path_to_image_folder, str(_award_year))\n",
    "    # if not os.path.exists(_path_to_image_folder):\n",
    "    #     os.makedirs(_path_to_image_folder, exist_ok=True)\n",
    "    target_images = []\n",
    "    target_dogs = []\n",
    "    all_images = {}\n",
    "    all_dogImages = {}\n",
    "\n",
    "    for i in range(25):\n",
    "        dict_im = {i: []}\n",
    "        all_images.update(dict_im)\n",
    "        dict_dog = {i: []}\n",
    "        all_dogImages.update(dict_dog)\n",
    "\n",
    "    counter = 0\n",
    "    for idx, im in enumerate(images):\n",
    "        _index = int(_award_year_list[idx]) - 1999\n",
    "        all_images[_index].append(im)\n",
    "    \n",
    "    for idx, dog in enumerate(dog_images):\n",
    "        _index = int(_award_year_list[idx]) - 1999\n",
    "        all_dogImages[_index].append(dog)\n",
    "\n",
    "    image_list = []\n",
    "    dog_list = []\n",
    "\n",
    "    for key in all_images.keys():\n",
    "        for im in all_images[key]:\n",
    "            image_list.append(im)\n",
    "\n",
    "    for key in all_dogImages.keys():\n",
    "        for im in all_dogImages[key]:\n",
    "            dog_list.append(im)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(100., 100.))\n",
    "    columns = 10\n",
    "    rows = 21\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(rows, columns), axes_pad=0.2,)            \n",
    "    \n",
    "    counter = 0    \n",
    "    for ax, im in zip(grid, dog_list):\n",
    "        # ax.set_title(\"image {}\".format(counter), fontsize = 50)\n",
    "        ax.imshow(im.squeeze(), 'gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.axis('off')\n",
    "    plt.autoscale()\n",
    "    if save_figure:\n",
    "        b_name = \"Dataset_DOG.png\"\n",
    "        f_name = os.path.join(path_to_image_folder, b_name)\n",
    "        plt.savefig(f_name)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    fig1 = plt.figure(figsize=(100., 100.))\n",
    "    columns = 10\n",
    "    rows = 21\n",
    "    grid1 = ImageGrid(fig1, 111, nrows_ncols=(rows, columns), axes_pad=0.2,)\n",
    "\n",
    "    for ax, im in zip(grid1, image_list):\n",
    "        # ax.set_title(\"image {}\".format(counter), fontsize = 50)\n",
    "        ax.imshow(im.squeeze(), 'gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.axis('off')\n",
    "    plt.autoscale()\n",
    "    if save_figure:\n",
    "        b_name = \"Dataset_OriginalImage.png\"\n",
    "        f_name = os.path.join(path_to_image_folder, b_name)\n",
    "        plt.savefig(f_name)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_ALLDatasetVis(_memory_dict['lda'], 'lda', save_figure=_save_figure, path_to_image_folder=_path_to_image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of latent structure mapping\n",
    "Visualize latent structure mapping with a class(building) label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_ALLvisualization_dict(data_dict, technique: str, colors_list, building_year_ordered_list, save_figure, path_to_image_folder, ex_name):\n",
    "    fig, ax = plt.subplots(figsize=(100,100))\n",
    "    ax.set_title(technique)\n",
    "\n",
    "    indices = data_dict['id']\n",
    "    coordinate = data_dict['coordinate']\n",
    "    bname_list = data_dict['name']\n",
    "    images = data_dict['original_image']\n",
    "    dog_images = data_dict['dog_image']\n",
    "    _award_year_list = data_dict['award_year']\n",
    "            \n",
    "    counter = 0\n",
    "    for idx, (x, y) in enumerate(coordinate):\n",
    "        _index = int(_award_year_list[idx]) - 1999\n",
    "        ax.scatter(x, y, s = 10000, color = colors_list[_index])\n",
    "        counter = counter + 1\n",
    "    patches = []\n",
    "    for i, color in enumerate(colors_list):\n",
    "        patch = mpatches.Patch(color=color)\n",
    "        patches.append(patch)\n",
    "\n",
    "    ax.update_datalim(coordinate)\n",
    "    ax.autoscale()\n",
    "    ax.set_title(technique, fontsize = 100)\n",
    "    plt.legend(patches, building_year_ordered_list, fontsize = 50)\n",
    "    if save_figure:\n",
    "        b_name = technique+\"_\"+ex_name+\"_2dscatterMap.png\"\n",
    "        f_name = os.path.join(_path_to_image_folder, b_name)\n",
    "        plt.savefig(f_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign different color to each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "colors = mcolors.CSS4_COLORS\n",
    "print(len(colors.keys()))\n",
    "_color_list = []\n",
    "for key in colors.keys():\n",
    "    _color_list.append(colors[key])\n",
    "    \n",
    "colors_list = []\n",
    "cnt = 15\n",
    "while cnt < 148:\n",
    "    colors_list.append(_color_list[cnt])\n",
    "    cnt = cnt + int(148/25)\n",
    "\n",
    "print(colors_list)\n",
    "print(len(colors_list))\n",
    "colors_list = colors_list[:25]\n",
    "print(len(colors_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of our latent structure mapping\n",
    "- Download .pckl for the latent structure mapping that we obtained by this process\n",
    "        - Download link [here](https://drive.google.com/file/d/12XoA7dlHa_QvZEdo6lxrS5G7bu8ClACT/view?usp=sharing)\n",
    "- Save it ./memory\n",
    "- The latent structure mapping that we obtained by this process can be visualized by loading following .pckl in memory folder\n",
    "        - '1508-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select specific ex_name\n",
    "# Following ex_name which contains the latent structure mapping used for my semantic understanding of 21 architectural projects\n",
    "ex_name = '1508-2' \n",
    "_temp_path_to_memeory = os.path.join(os.getcwd(), 'memory')\n",
    "_p_name = ex_name+'_data_dict.pckl'\n",
    "_memory_dict = load_from_memory(path_to_memory=_temp_path_to_memeory, pickle_fname=_p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_ALLvisualization_dict(_memory_dict['lda'], technique = 'lda', colors_list = colors_list, building_year_ordered_list=list_building_name ,save_figure = _save_figure, path_to_image_folder=\"\", ex_name = ex_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic understanding\n",
    "## Analysis 1\n",
    "- Compact Cluster vs. Non-compact Cluster\n",
    "\n",
    "Our first observation pertained to how the images of each architectural project are distributed in the latent structure mapping. By applying Principal Component Analysis (PCA) to each class, I can obtain the two most variant direction and their variance, which can describe the distribution of plots for each building. From this result, I can find which type each cluster belongs to, a compact cluster or a non-compact cluster. To determine those profiles of a cluster, I need to derive the maximum variance ratio (MVR). MVR can be derived as follows where maximum variance $\\sigma_{i, \\text{max}}^2$ and minimum variance $\\sigma_{i, \\text{min}}^2$ of ith building. \n",
    "\n",
    "$$ \\text{MVR}(i) = \\frac{\\sigma_{i, \\text{max}}^2}{\\sigma_{i, \\text{max}}^2 + \\sigma_{i, \\text{min}}^2} (\\leqq1.0, \\text{in 2D})$$\n",
    "\n",
    "As the MVR gets close to 0.5, a cluster becomes a compact cluster and it will be close to isotropic Gaussian distribution. On the other hand, as the MVR gets close to 1.0, a cluster becomes a non-compact cluster and it will be an anisotropic Gaussian distribution. By sorting the maximum variance ratio among 21 architectural projects as shown in \\autoref{fig:SortedMaxRatio}, I could derive the top 3 compact clusters and the top 3 non-compact clusters in the latent structure mapping. These clusters can be seen with segmentation in \\autoref{fig:Top3Compact, fig:Top3Noncompact}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get centroid of each class cluster and its variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_mean_variance(data_dict, technique: str, colors_list, building_year_ordered_list, save_figure, path_to_image_folder, ex_name):\n",
    "    indices = data_dict['id']\n",
    "    coordinate = data_dict['coordinate']\n",
    "    bname_list = data_dict['name']\n",
    "    images = data_dict['original_image']\n",
    "    dog_images = data_dict['dog_image']\n",
    "    _award_year_list = data_dict['award_year']\n",
    "            \n",
    "    _counter = 0\n",
    "    coords = {}\n",
    "\n",
    "    class_num = int(len(coordinate)/test_num_image_per_class)\n",
    "\n",
    "    for i in range(len(building_year_ordered_list)):\n",
    "        coords.update({i : []})\n",
    "\n",
    "    for idx, (x, y) in enumerate(coordinate):\n",
    "        _index = int(_award_year_list[idx]) - 1999\n",
    "        coords[_index].append([x, y])  \n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain centroid coordinates for 21 architectural projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_coords = Get_mean_variance(_memory_dict['lda'], technique = 'lda', colors_list = colors_list, building_year_ordered_list=list_building_name ,save_figure = _save_figure, path_to_image_folder=\"\", ex_name = ex_name)\n",
    "mean_x ={}\n",
    "mean_y ={}\n",
    "\n",
    "for i in range(22):\n",
    "    mean_x.update({i : 0})\n",
    "    mean_y.update({i : 0})\n",
    "\n",
    "for key in centroid_coords.keys():\n",
    "    if key > 20:\n",
    "        break\n",
    "    for i in range(10):\n",
    "        mean_x[key] = mean_x[key]+centroid_coords[key][i][0]\n",
    "        mean_y[key] = mean_y[key]+centroid_coords[key][i][1]\n",
    "\n",
    "for i in range(21):\n",
    "    mean_x[i] = mean_x[i]/10\n",
    "    mean_y[i] = mean_y[i]/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means(colors_list, building_year_ordered_list, mean_x, mean_y, techniques = 'lda'):\n",
    "    fig, ax = plt.subplots(figsize=(100,100))\n",
    "    ax.set_title(techniques)\n",
    "\n",
    "    class_num = 21\n",
    "\n",
    "    for i, name in enumerate(building_year_ordered_list):\n",
    "        if i > 21:\n",
    "            break\n",
    "        ax.scatter(mean_x[i], mean_y[i], s = 10000, color = colors_list[i])\n",
    "        ax.annotate(i, (mean_x[i], mean_y[i]), fontsize = 50)\n",
    "    \n",
    "    patches = []\n",
    "\n",
    "    for i, color in enumerate(colors_list[:class_num]):\n",
    "        patch = mpatches.Patch(color=color)\n",
    "        patches.append(patch)\n",
    "    \n",
    "    award_year = 1999\n",
    "    legend_nameList = []\n",
    "\n",
    "    for id in range(len(building_year_ordered_list)):\n",
    "        temp = building_year_ordered_list[id]\n",
    "        legend_nameList.append(str(id) +\". \"+ temp)\n",
    "\n",
    "    ax.autoscale()\n",
    "    ax.set_title(techniques, fontsize = 100)\n",
    "    plt.legend(patches[:class_num], legend_nameList[:class_num], fontsize = 75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means(colors_list=colors_list, building_year_ordered_list=list_building_name, mean_x=mean_x, mean_y=mean_y, techniques='lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA to each class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_variance_ratio = []\n",
    "pca_singular_values = []\n",
    "\n",
    "for key in centroid_coords.keys():\n",
    "    if key > 20:\n",
    "        break\n",
    "    _array = np.array(centroid_coords[key])\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(_array)\n",
    "    pca_variance_ratio.append(pca.explained_variance_ratio_)\n",
    "    pca_singular_values.append(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creation of pandas for comaprison of MVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dict_pca_variance_ratio = {}\n",
    "dict_singular_values = {}\n",
    "pca_results = {}\n",
    "ids = []\n",
    "max_var = []\n",
    "min_var = []\n",
    "max_ratio = []\n",
    "min_ratio = []\n",
    "names = []\n",
    "mx_coords = []\n",
    "my_coords = []\n",
    "\n",
    "for id, b_name in enumerate(list_building_name):\n",
    "    if id > 20:\n",
    "        break\n",
    "    ids.append(id)\n",
    "    names.append(b_name)\n",
    "    max_var.append(pca_singular_values[id][0])\n",
    "    min_var.append(pca_singular_values[id][1])\n",
    "    max_ratio.append(pca_variance_ratio[id][0])\n",
    "    min_ratio.append(pca_variance_ratio[id][1])\n",
    "\n",
    "data = {'id': ids, 'name': names, 'max_var': max_var, 'min_var': min_var, 'max_ratio': max_ratio, 'min_ratio': min_ratio}\n",
    "df = pd.DataFrame(data)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of maximum variance ratio (MVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['max_ratio'])\n",
    "print(sorted_df)\n",
    "sorted_df.plot(kind='bar', x = 'name', y = 'max_ratio', color = 'blue')\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Maximum variance ratio (sorted)')\n",
    "plt.xlabel('buiding name')\n",
    "plt.ylabel('variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other comparisons\n",
    "Comparison of maximum variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['max_var'])\n",
    "print(sorted_df)\n",
    "sorted_df.plot(kind='bar', x = 'name', y = 'max_var')\n",
    "plt.ylim([0, 15])\n",
    "plt.title('Maximum variance (sorted)')\n",
    "plt.xlabel('buiding name')\n",
    "plt.ylabel('variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of minimum variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['min_var'])\n",
    "print(sorted_df)\n",
    "sorted_df.plot(kind='bar', x = 'name', y = 'min_var', color = 'green')\n",
    "plt.ylim([0, 15])\n",
    "plt.title('Minimum variance (sorted)')\n",
    "plt.xlabel('buiding name')\n",
    "plt.ylabel('variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of minimum variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['min_ratio'])\n",
    "print(sorted_df)\n",
    "sorted_df.plot(kind='bar', x = 'name', y = 'min_ratio', color = 'red')\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Minimum variance ratio (sorted)')\n",
    "plt.xlabel('buiding name')\n",
    "plt.ylabel('variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means_separation(colors_list, building_year_ordered_list, mean_x, mean_y, techniques = 'lda'):\n",
    "    fig, ax = plt.subplots(figsize=(100,100))\n",
    "    ax.set_title(techniques)\n",
    "\n",
    "    class_num = 21\n",
    "\n",
    "    for i, name in enumerate(building_year_ordered_list):\n",
    "        if i > 20:\n",
    "            break\n",
    "        if i + 1999 > 2009:\n",
    "            ax.scatter(mean_x[i], mean_y[i], s = 10000, color = 'red')\n",
    "        elif i+1999 == 2009:\n",
    "            ax.scatter(mean_x[i], mean_y[i], s = 10000, color = 'yellow')\n",
    "        else:\n",
    "            ax.scatter(mean_x[i], mean_y[i], s = 10000, color = 'green')\n",
    "        ax.annotate(i, (mean_x[i], mean_y[i]), fontsize = 50)\n",
    "\n",
    "\n",
    "    \n",
    "    patches = []\n",
    "\n",
    "    patch = mpatches.Patch(color='red')\n",
    "    patches.append(patch)\n",
    "    patch = mpatches.Patch(color='yellow')\n",
    "    patches.append(patch)\n",
    "    patch = mpatches.Patch(color='green')\n",
    "    patches.append(patch)\n",
    "    \n",
    "    award_year = 1999\n",
    "    legend_nameList = []\n",
    "\n",
    "    legend_nameList.append(\"before 2009\")\n",
    "    legend_nameList.append(\"2009\")\n",
    "    legend_nameList.append(\"after 2019\")\n",
    "\n",
    "    ax.autoscale()\n",
    "    ax.set_title(techniques, fontsize = 100)\n",
    "    plt.legend(patches[:class_num], legend_nameList[:class_num], fontsize = 75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_separation(colors_list=colors_list, building_year_ordered_list=list_building_name, mean_x=mean_x, mean_y=mean_y, techniques='lda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
